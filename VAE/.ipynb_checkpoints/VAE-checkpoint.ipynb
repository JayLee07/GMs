{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import itertools\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from scipy.misc import imsave\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as v_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'MNIST'\n",
    "        self.dataroot = '/data/jehyuk/imgdata'\n",
    "        self.workers = 2\n",
    "        self.n_gpu = 1\n",
    "        self.batchsize = 128\n",
    "        self.maxepoch = 5\n",
    "        self.imagesize = 28\n",
    "        self.lr = 0.0001\n",
    "        self.use_cuda = True\n",
    "        self.n_h = [128, 64]\n",
    "        self.n_z = 20\n",
    "        self.dims = [self.imagesize*self.imagesize, self.n_h, self.n_z]\n",
    "        self.result_dir = '/home/jehyuk/GenerativeModels/VAE/results/VAE/' + self.dataset\n",
    "        self.save_dir = '/home/jehyuk/GenerativeModels/VAE/models/VAE/' + self.dataset\n",
    "        self.n_sample = 25\n",
    "\n",
    "        if self.dataset == 'MNIST':\n",
    "            self.n_channels = 1\n",
    "        elif self.dataset == 'Fashion-MNIST':\n",
    "            self.n_channels = 1\n",
    "        else:\n",
    "            self.n_channles = 3\n",
    "            \n",
    "opt = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataroot = opt.dataroot, dataset=opt.dataset):\n",
    "    data_folder = os.path.join(dataroot, dataset)\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    transform = transforms.Compose([transforms.Scale(opt.imagesize),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "    if dataset == 'MNIST':\n",
    "        trn_data = dset.MNIST(data_folder, train=True, transform=transform, download=True)\n",
    "        tst_data = dset.MNIST(data_folder, train=False, transform=transform, download=True)\n",
    "        n_channels = 1\n",
    "    elif dataset == 'Fashion-MNIST':\n",
    "        trn_data = dset.FashionMNIST(data_folder, train=True, transform=transform, download=True)\n",
    "        tst_data = dset.FashionMNIST(data_folder, train=False, transform=transform, download=True)\n",
    "        n_channels = 1\n",
    "        pass\n",
    "    elif dataset == 'CIFAR10':\n",
    "        trn_data = dset.cifar.CIFAR10(data_folder, train=True, transform=transform, download=True)\n",
    "        tst_data = dset.cifar.CIFAR10(data_folder, train=False, transform=transform, download=True)\n",
    "        n_channels = 3\n",
    "    trn_loader = utils.data.DataLoader(trn_data, batch_size=opt.batchsize, shuffle=True, num_workers=opt.workers, drop_last=True)\n",
    "    tst_loader = utils.data.DataLoader(tst_data, batch_size=opt.batchsize, shuffle=False, num_workers=opt.workers, drop_last=True)\n",
    "    return trn_loader, tst_loader, n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    image = np.squeeze(merge(images, size))\n",
    "    return imsave(image_path, image)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    if (images.shape[3] in (3,4)):\n",
    "        c = images.shape[3]\n",
    "        img = np.zeros((h * size[0], w * size[1], c))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "        return img\n",
    "    elif images.shape[3]==1:\n",
    "        img = np.zeros((h * size[0], w * size[1]))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6931\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmploss = nn.BCELoss()\n",
    "tmploss(Variable(0.5*torch.ones(10,1)), Variable(torch.ones(10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BCELoss(r, x):\n",
    "    Loss = nn.BCELoss(size_average=False)\n",
    "    return Loss(r,x)\n",
    "\n",
    "def KL_divergence_normal(mu, logvar):\n",
    "    return 0.5*(1. + logvar - mu**2 - torch.exp(logvar))\n",
    "\n",
    "class VI(nn.Module):\n",
    "    def __init__(self, recon_prob, KL_div):\n",
    "        super(VI, self).__init__()\n",
    "        self.recon_prob = recon_prob\n",
    "        self.KL_div = KL_div\n",
    "    def forward(self, x_hat, x, mu, logvar):\n",
    "        logL = self.recon_prob(x_hat, x)\n",
    "        KLD = torch.sum(self.KL_div(mu, logvar))\n",
    "        return logL - KLD\n",
    "    \n",
    "class VI_with_labels(nn.Module):\n",
    "    def __init__(self, recon_prob, KL_div, prior_y):\n",
    "        super(VI_with_labels, self).__init__(recon_prob, KL_div)\n",
    "        self.prior_y = prior_y\n",
    "    def forward(self, x_hat, x, y, latent):\n",
    "        log_prior_y = self.prior_y(y)\n",
    "        logL = self.recon_prob(x_hat, x)\n",
    "        KL_div = [torch.sum(self.KL_div(mu, logvar), dim=-1) for _, mu, logvar in latent]\n",
    "        return logL + log_prior_y + sum(KL_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticGaussian(nn.Module):\n",
    "    def __init__(self, h_dim, z_dim):\n",
    "        super(StochasticGaussian, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.mu = nn.Linear(h_dim, z_dim)\n",
    "        self.logvar = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        eps = Variable(torch.randn(mu.size())).cuda()\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        z = mu + eps * std\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        x_dim = self.opt.imagesize * self.opt.imagesize\n",
    "        h_dim = self.opt.n_h\n",
    "        z_dim = self.opt.n_z\n",
    "        neurons = [x_dim, *h_dim]\n",
    "        layers = [nn.Linear(neurons[i], neurons[i+1]) for i in range(0, len(neurons)-1)]\n",
    "        self.h = nn.ModuleList(layers)\n",
    "        self.sample = StochasticGaussian(h_dim[-1], z_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(self.opt.batchsize, -1)\n",
    "        for i, layer in enumerate(self.h):\n",
    "            x = layer(x)\n",
    "            if i < len(self.h) -1:\n",
    "                x = self.relu(x)\n",
    "        z, mu, logvar = self.sample(x)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        x_dim = self.opt.imagesize * self.opt.imagesize\n",
    "        h_dim = list(reversed(self.opt.n_h))\n",
    "        z_dim = self.opt.n_z\n",
    "        neurons = [z_dim, *h_dim]\n",
    "        layers = [nn.Linear(neurons[i], neurons[i+1]) for i in range(0, len(neurons)-1)]\n",
    "        self.h = nn.ModuleList(layers)\n",
    "        self.recon = nn.Linear(h_dim[-1], x_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        for i, layer in enumerate(self.h):\n",
    "            z = layer(z)\n",
    "            if i < len(self.h)-1:\n",
    "                z = self.relu(z)\n",
    "        x_hat = self.sigmoid(self.recon(z))\n",
    "        x_hat = x_hat.view(-1, self.opt.imagesize, self.opt.imagesize, self.opt.n_channels)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    \n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.trn_loader, self.tst_loader, self.n_channels = load_dataset(self.opt.dataroot, self.opt.dataset)\n",
    "        self.is_cuda = torch.cuda.is_available()\n",
    "        \n",
    "        self.encoder = Encoder(self.opt)\n",
    "        self.decoder = Decoder(self.opt)\n",
    "        if self.is_cuda and self.opt.use_cuda:\n",
    "            self.encoder, self.decoder = self.encoder.cuda(), self.decoder.cuda()\n",
    "        \n",
    "        self.objective = VI(BCELoss, KL_divergence_normal) # Maximize (Variational lower bound) = minimize -(Variational lower bound)\n",
    "        self.trainable_params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optim = torch.optim.Adam(params=self.trainable_params, lr=self.opt.lr, betas=(0.5, 0.999))\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.trainable_params:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, (z, mu, logvar)\n",
    "    \n",
    "    def train(self):\n",
    "        self._initialize_weights()\n",
    "        self.loss_dict=dict()\n",
    "        self.loss_dict['loss'] = list()\n",
    "        \n",
    "        if self.is_cuda and self.opt.use_cuda:\n",
    "            self.encoder, self.decoder = self.encoder.cuda(), self.decoder.cuda()\n",
    "        \n",
    "        print('------------------Start training------------------')\n",
    "        self.optim.zero_grad()\n",
    "        for epoch in range(self.opt.maxepoch):\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            print(\">>>>Epoch: {}\".format(epoch+1))\n",
    "            start_time = time.time()\n",
    "            for iter_num, (image, label) in enumerate(self.trn_loader):\n",
    "                x = Variable(image)\n",
    "                if self.is_cuda:\n",
    "                    x = x.cuda()\n",
    "                x_hat, (_, z_mu, z_logvar) = self.forward(x)\n",
    "                loss = -self.objective(x_hat, x, z_mu, z_logvar)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "            print(\">>>>Time for epoch {}: {:.2f}, loss: {:.3f}\".format(epoch+1, time.time()-start_time, loss.data[0]))\n",
    "            self.visualize_results(epoch+1)\n",
    "        self.save_model()\n",
    "            \n",
    "    \n",
    "    def visualize_results(self, epoch, fix=False):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        for iter_num, (image, label) in enumerate(self.tst_loader):\n",
    "            x = Variable(image)\n",
    "            if self.is_cuda:\n",
    "                x = x.cuda()\n",
    "            x_hat, (_,_,_) = self.forward(x)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(self.opt.n_sample)))\n",
    "#         if fix:\n",
    "#             x_hat = self.decoder.forward(self.sample_z)\n",
    "#         else:\n",
    "#             sample_z = Variable(0.1*torch.randn((self.opt.n_sample, self.opt.n_z)), volatile=True)\n",
    "#             if self.is_cuda and self.opt.use_cuda:\n",
    "#                 sample_z = sample_z.cuda()\n",
    "#             x_hat = self.decoder.forward(self.sample_z)\n",
    "        \n",
    "        if self.is_cuda and self.opt.use_cuda:\n",
    "            x_hat = x_hat.cpu().data.numpy().reshape(-1, self.opt.imagesize, self.opt.imagesize, self.opt.n_channels)\n",
    "        else:\n",
    "            x_hat = x_hat.data.numpy().reshape(-1, self.opt.imagesize, self.opt.imagesize, self.opt.n_channels)\n",
    "        \n",
    "        save_images(x_hat[:image_frame_dim * image_frame_dim,:,:,:], [image_frame_dim, image_frame_dim], self.opt.result_dir + '/' + 'VAE_epoch%03d' %epoch  + '.png')\n",
    "        \n",
    "    def save_model(self):\n",
    "        if not os.path.exists(self.opt.save_dir):\n",
    "            os.makedirs(self.opt.save_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(self.opt.save_dir, 'VAE.pkl'))\n",
    "        with open(os.path.join(self.opt.save_dir, 'loss_dict'), 'wb') as f:\n",
    "            pickle.dump(self.loss_dict, f)\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.load_state_dict(torch.load(os.path.join(self.opt.save_dir, 'VAE.pkl')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Start training------------------\n",
      ">>>>Epoch: 1\n",
      ">>>>Time for epoch 1: 13.60, loss: -969005895561592700928.000\n",
      ">>>>Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/mnist.py\", line 76, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 71, in to_tensor\n",
      "    img = img.view(pic.size[1], pic.size[0], nchannel)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-18734c3265d0>\", line 2, in <module>\n",
      "    vae.train()\n",
      "  File \"<ipython-input-10-8a0ea085e1e3>\", line 44, in train\n",
      "    for iter_num, (image, label) in enumerate(self.trn_loader):\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/resource_sharer.py\", line 88, in get_connection\n",
      "    c.send((key, os.getpid()))\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/multiprocessing/reduction.py\", line 39, in __init__\n",
      "    super().__init__(*args)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/inspect.py\", line 736, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/inspect.py\", line 705, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/inspect.py\", line 684, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "  File \"/home/jehyuk/.conda/envs/jehyuk/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 14136) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "vae = VAE(opt)\n",
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"/home/jehyuk/GenerativeModels/VAE/results/VAE/MNIST/VAE_epoch004.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
