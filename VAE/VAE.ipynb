{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import itertools\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from scipy.misc import imsave\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as v_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'MNIST'\n",
    "        self.dataroot = '/data/jehyuk/imgdata'\n",
    "        self.workers = 2\n",
    "        self.n_gpu = 1\n",
    "        self.batchsize = 128\n",
    "        self.maxepoch = 5\n",
    "        self.imagesize = 28\n",
    "        self.lr = 0.0001\n",
    "        self.use_cuda = True\n",
    "        self.n_h = [128, 64]\n",
    "        self.n_z = 20\n",
    "        self.dims = [self.imagesize*self.imagesize, self.n_h, self.n_z]\n",
    "        self.result_dir = '/home/jehyuk/GenerativeModels/VAE/results/VAE/' + self.dataset\n",
    "        self.save_dir = '/home/jehyuk/GenerativeModels/VAE/models/VAE/' + self.dataset\n",
    "        self.n_sample = 25\n",
    "\n",
    "        if self.dataset == 'MNIST':\n",
    "            self.n_channels = 1\n",
    "        elif self.dataset == 'Fashion-MNIST':\n",
    "            self.n_channels = 1\n",
    "        else:\n",
    "            self.n_channles = 3\n",
    "            \n",
    "opt = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataroot = opt.dataroot, dataset=opt.dataset):\n",
    "    data_folder = os.path.join(dataroot, dataset)\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    transform = transforms.Compose([transforms.Scale(opt.imagesize),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "    if dataset == 'MNIST':\n",
    "        trn_data = dset.MNIST(data_folder, train=True, transform=transform, download=True)\n",
    "        tst_data = dset.MNIST(data_folder, train=False, transform=transform, download=True)\n",
    "        n_channels = 1\n",
    "    elif dataset == 'Fashion-MNIST':\n",
    "        trn_data = dset.FashionMNIST(data_folder, train=True, transform=transform, download=True)\n",
    "        tst_data = dset.FashionMNIST(data_folder, train=False, transform=transform, download=True)\n",
    "        n_channels = 1\n",
    "        pass\n",
    "    elif dataset == 'CIFAR10':\n",
    "        trn_data = dset.cifar.CIFAR10(data_folder, train=True, transform=transform, download=True)\n",
    "        tst_data = dset.cifar.CIFAR10(data_folder, train=False, transform=transform, download=True)\n",
    "        n_channels = 3\n",
    "    trn_loader = utils.data.DataLoader(trn_data, batch_size=opt.batchsize, shuffle=True, num_workers=opt.workers, drop_last=True)\n",
    "    tst_loader = utils.data.DataLoader(tst_data, batch_size=opt.batchsize, shuffle=False, num_workers=opt.workers, drop_last=True)\n",
    "    return trn_loader, tst_loader, n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    image = np.squeeze(merge(images, size))\n",
    "    return imsave(image_path, image)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    if (images.shape[3] in (3,4)):\n",
    "        c = images.shape[3]\n",
    "        img = np.zeros((h * size[0], w * size[1], c))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "        return img\n",
    "    elif images.shape[3]==1:\n",
    "        img = np.zeros((h * size[0], w * size[1]))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BCELoss(r, x):\n",
    "    Loss = nn.BCELoss(size_average=False)\n",
    "    return Loss(r,x)\n",
    "\n",
    "def KL_divergence_normal(mu, logvar):\n",
    "    return 0.5*(1. + logvar - mu**2 - torch.exp(logvar))\n",
    "\n",
    "class VI(nn.Module):\n",
    "    def __init__(self, recon_prob, KL_div):\n",
    "        super(VI, self).__init__()\n",
    "        self.recon_prob = recon_prob\n",
    "        self.KL_div = KL_div\n",
    "    def forward(self, x_hat, x, mu, logvar):\n",
    "        logL = self.recon_prob(x_hat, x)\n",
    "        KLD = torch.sum(self.KL_div(mu, logvar))\n",
    "        return logL - KLD\n",
    "    \n",
    "class VI_with_labels(nn.Module):\n",
    "    def __init__(self, recon_prob, KL_div, prior_y):\n",
    "        super(VI_with_labels, self).__init__(recon_prob, KL_div)\n",
    "        self.prior_y = prior_y\n",
    "    def forward(self, x_hat, x, y, latent):\n",
    "        log_prior_y = self.prior_y(y)\n",
    "        logL = self.recon_prob(x_hat, x)\n",
    "        KL_div = [torch.sum(self.KL_div(mu, logvar), dim=-1) for _, mu, logvar in latent]\n",
    "        return logL + log_prior_y + sum(KL_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticGaussian(nn.Module):\n",
    "    def __init__(self, h_dim, z_dim):\n",
    "        super(StochasticGaussian, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.mu = nn.Linear(h_dim, z_dim)\n",
    "        self.logvar = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        eps = Variable(torch.randn(mu.size())).cuda()\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        z = mu + eps * std\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        x_dim = self.opt.imagesize * self.opt.imagesize\n",
    "        h_dim = self.opt.n_h\n",
    "        z_dim = self.opt.n_z\n",
    "        neurons = [x_dim, *h_dim]\n",
    "        layers = [nn.Linear(neurons[i], neurons[i+1]) for i in range(0, len(neurons)-1)]\n",
    "        self.h = nn.ModuleList(layers)\n",
    "        self.sample = StochasticGaussian(h_dim[-1], z_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(self.opt.batchsize, -1)\n",
    "        for i, layer in enumerate(self.h):\n",
    "            x = layer(x)\n",
    "            if i < len(self.h) -1:\n",
    "                x = self.relu(x)\n",
    "        z, mu, logvar = self.sample(x)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        x_dim = self.opt.imagesize * self.opt.imagesize\n",
    "        h_dim = list(reversed(self.opt.n_h))\n",
    "        z_dim = self.opt.n_z\n",
    "        neurons = [z_dim, *h_dim]\n",
    "        layers = [nn.Linear(neurons[i], neurons[i+1]) for i in range(0, len(neurons)-1)]\n",
    "        self.h = nn.ModuleList(layers)\n",
    "        self.recon = nn.Linear(h_dim[-1], x_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        for i, layer in enumerate(self.h):\n",
    "            z = layer(z)\n",
    "            if i < len(self.h)-1:\n",
    "                z = self.relu(z)\n",
    "        x_hat = self.sigmoid(self.recon(z))\n",
    "        x_hat = x_hat.view(-1, self.opt.imagesize, self.opt.imagesize, self.opt.n_channels)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    \n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.trn_loader, self.tst_loader, self.n_channels = load_dataset(self.opt.dataroot, self.opt.dataset)\n",
    "        self.is_cuda = torch.cuda.is_available()\n",
    "        \n",
    "        self.encoder = Encoder(self.opt)\n",
    "        self.decoder = Decoder(self.opt)\n",
    "        if self.is_cuda and self.opt.use_cuda:\n",
    "            self.encoder, self.decoder = self.encoder.cuda(), self.decoder.cuda()\n",
    "        \n",
    "        self.objective = VI(BCELoss, KL_divergence_normal)\n",
    "        self.trainable_params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optim = torch.optim.Adam(params=self.trainable_params, lr=self.opt.lr, betas=(0.5, 0.999))\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.trainable_params:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, (z, mu, logvar)\n",
    "    \n",
    "    def train(self):\n",
    "        self._initialize_weights()\n",
    "        self.loss_dict=dict()\n",
    "        self.loss_dict['loss'] = list()\n",
    "        \n",
    "        if self.is_cuda and self.opt.use_cuda:\n",
    "            self.encoder, self.decoder = self.encoder.cuda(), self.decoder.cuda()\n",
    "        \n",
    "        print('------------------Start training------------------')\n",
    "        self.optim.zero_grad()\n",
    "        for epoch in range(self.opt.maxepoch):\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            print(\">>>>Epoch: {}\".format(epoch+1))\n",
    "            start_time = time.time()\n",
    "            for iter_num, (image, label) in enumerate(self.trn_loader):\n",
    "                x = Variable(image)\n",
    "                if self.is_cuda:\n",
    "                    x = x.cuda()\n",
    "                x_hat, (_, z_mu, z_logvar) = self.forward(x)\n",
    "                loss = -self.objective(x_hat, x, z_mu, z_logvar)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "            print(\">>>>Time for epoch {}: {:.2f}, loss: {:.3f}\".format(epoch+1, time.time()-start_time, loss.data[0]))\n",
    "            self.visualize_results(epoch+1)\n",
    "        self.save_model()\n",
    "            \n",
    "    \n",
    "    def visualize_results(self, epoch, fix=False):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        for iter_num, (image, label) in enumerate(self.tst_loader):\n",
    "            x = Variable(image)\n",
    "            if self.is_cuda:\n",
    "                x = x.cuda()\n",
    "            x_hat, (_,_,_) = self.forward(x)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(self.opt.n_sample)))\n",
    "        \n",
    "        if self.is_cuda and self.opt.use_cuda:\n",
    "            x_hat = x_hat.cpu().data.numpy().reshape(-1, self.opt.imagesize, self.opt.imagesize, self.opt.n_channels)\n",
    "        else:\n",
    "            x_hat = x_hat.data.numpy().reshape(-1, self.opt.imagesize, self.opt.imagesize, self.opt.n_channels)\n",
    "        \n",
    "        save_images(x_hat[:image_frame_dim * image_frame_dim,:,:,:], [image_frame_dim, image_frame_dim], self.opt.result_dir + '/' + 'VAE_epoch%03d' %epoch  + '.png')\n",
    "        \n",
    "    def save_model(self):\n",
    "        if not os.path.exists(self.opt.save_dir):\n",
    "            os.makedirs(self.opt.save_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(self.opt.save_dir, 'VAE.pkl'))\n",
    "        with open(os.path.join(self.opt.save_dir, 'loss_dict'), 'wb') as f:\n",
    "            pickle.dump(self.loss_dict, f)\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.load_state_dict(torch.load(os.path.join(self.opt.save_dir, 'VAE.pkl')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Start training------------------\n",
      ">>>>Epoch: 1\n",
      ">>>>Time for epoch 1: 16.59, loss: -2061772.500\n",
      ">>>>Epoch: 2\n",
      ">>>>Time for epoch 2: 15.59, loss: -2015149.000\n",
      ">>>>Epoch: 3\n",
      ">>>>Time for epoch 3: 15.86, loss: -2039001.625\n",
      ">>>>Epoch: 4\n",
      ">>>>Time for epoch 4: 15.77, loss: -2062462.250\n",
      ">>>>Epoch: 5\n",
      ">>>>Time for epoch 5: 16.33, loss: -2075702.000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-18734c3265d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-7618e49746c4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>>>Time for epoch {}: {:.2f}, loss: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7618e49746c4>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VAE.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "vae = VAE(opt)\n",
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIwAAACMCAAAAACLqx7iAAABHklEQVR4nO3ZvyvEcRjA8ctfYHOD\nQd3CpqSOvik/cneSxV10yKIoJQxGSjJhM7ApV7LeZLOQmyQWYcA/4D9g/fjW+zN8F6n3a3t69/wB\nT08uJ0mS/qX8ALficmRvjNvlN7fOJJzafsehd16sR1rxkVvpgFv5g5skSZIkSRnVdrl1jXAbXeV2\n/MVtuxFOqVt78Z4Xy8/c5l65JYfcpm+5SZIkSZKUUccEt9Imt70Lbmef3BZ2wil1a3ff8eLwNbfZ\nG25TDW4zD9wkSZIkScro6IXb+Aa3tchLuK/Kbb8ZTqlbe/CUF+cjd3hywq0a+XmvR5okSZIkSRnl\nK9wKW9za+7n1rkTaUjilbu2eFi9OnnMrvHGrXHGrPXGTJOnv/QAlfC5VvduDTQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"/home/jehyuk/GenerativeModels/VAE/results/VAE/MNIST/VAE_epoch005.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
